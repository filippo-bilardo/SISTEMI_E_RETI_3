# 8. QoS e Performance

La qualità del servizio (QoS) e le performance sono aspetti critici per garantire un'esperienza utente soddisfacente con le VPN. Questo capitolo esamina l'impatto delle VPN sulle prestazioni e come ottimizzarle.

## 8.1 Impatto delle VPN sulle prestazioni

### Overhead VPN

Ogni protocollo VPN aggiunge overhead che riduce bandwidth utile e aumenta latenza.

**Composizione overhead:**
```
Pacchetto originale: 1500 bytes (MTU Ethernet standard)

IPsec ESP Tunnel Mode:
├─ IP header esterno:      20 bytes
├─ ESP header:             8 bytes
├─ ESP IV:                 16 bytes (AES-CBC)
├─ IP header originale:    20 bytes
├─ TCP header:             20 bytes
├─ Payload:                1416 bytes ← ridotto!
├─ ESP padding:            1-16 bytes
├─ ESP trailer:            2 bytes
└─ ESP auth (ICV):         12-16 bytes

Totale overhead: ~84-100 bytes
Payload utile: ~1400-1416 bytes
Overhead %: 5.6-6.7%
```

**Overhead per protocollo:**
```
Protocollo          Overhead    MTU Effettivo    Note
─────────────────────────────────────────────────────────
Native (no VPN)     0 bytes     1500             Baseline
IPsec (ESP)         ~52 bytes   1448             Transport mode
IPsec (ESP+NAT-T)   ~60 bytes   1440             UDP encap
OpenVPN (UDP)       ~42 bytes   1458             Compresso
OpenVPN (TCP)       ~62 bytes   1438             Double encap
WireGuard           ~32 bytes   1468             Più efficiente
L2TP/IPsec          ~82 bytes   1418             Doppio tunnel
PPTP                ~50 bytes   1450             Legacy
```

### Fattori che impattano le performance

#### 1. Crittografia
```
CPU Usage by Cipher (per core):

AES-128-CBC (no AES-NI):  ~400 Mbps
AES-256-CBC (no AES-NI):  ~300 Mbps
AES-128-GCM (no AES-NI):  ~200 Mbps

AES-128-GCM (with AES-NI): ~10 Gbps
AES-256-GCM (with AES-NI): ~7 Gbps

ChaCha20-Poly1305:         ~1.5 Gbps (any CPU)
```

**Verifica supporto AES-NI:**
```bash
# Linux
grep -m1 -o aes /proc/cpuinfo && echo "AES-NI supported" || echo "Not supported"

# Check OpenVPN using AES-NI
lscpu | grep aes
```

#### 2. Protocollo di tunneling
```
Throughput comparison (1 Gbps link, AES-256-GCM):

WireGuard:     920 Mbps  (92% efficiency)
IPsec:         850 Mbps  (85%)
OpenVPN UDP:   780 Mbps  (78%)
OpenVPN TCP:   650 Mbps  (65%)  ← TCP-over-TCP problem
```

#### 3. Hardware
- **CPU**: numero di core, frequenza, AES-NI support
- **RAM**: buffering, session state
- **Network**: NIC quality, driver optimization
- **Storage**: logging (I/O intensive)

#### 4. Network conditions
- **Latency**: RTT incremento ~10-50ms
- **Jitter**: variabilità latenza
- **Packet loss**: retransmission overhead
- **Bandwidth**: saturazione link

## 8.2 Latenza e throughput

### Misurazione latenza

**Baseline vs VPN:**
```bash
# Baseline (without VPN)
ping -c 10 8.8.8.8

# Results:
# rtt min/avg/max/mdev = 10.2/12.5/15.3/1.8 ms

# With VPN
ping -c 10 192.168.1.10  # Internal resource via VPN

# Results:
# rtt min/avg/max/mdev = 25.4/30.2/38.1/4.2 ms

# Added latency: ~17-18 ms (encryption + routing)
```

**Componenti latenza VPN:**
```
Total Latency = Network_Latency + 
                Encryption_Latency + 
                Decryption_Latency + 
                Processing_Overhead

Example:
Network RTT:       10 ms
Encrypt (client):   2 ms
Decrypt (server):   2 ms
Processing:         3 ms
─────────────────────────
Total:             17 ms
```

### Throughput testing

**iperf3 test:**
```bash
# Server side (internal network)
iperf3 -s

# Client side (via VPN)
# Test TCP throughput
iperf3 -c 192.168.1.100 -t 60 -i 5

# Test UDP throughput
iperf3 -c 192.168.1.100 -u -b 100M -t 60

# Parallel streams (simulate multiple users)
iperf3 -c 192.168.1.100 -P 10 -t 30
```

**Expected results:**
```
Scenario                Throughput      Notes
───────────────────────────────────────────────────────
Direct connection       950 Mbps        Baseline
VPN (WireGuard)        880 Mbps        ~93% efficiency
VPN (IPsec)            820 Mbps        ~86%
VPN (OpenVPN UDP)      750 Mbps        ~79%
VPN (OpenVPN TCP)      600 Mbps        ~63% (TCP overhead)
```

### Latency-sensitive applications

**VoIP quality requirements:**
```
Metric              Acceptable    Good      Excellent
───────────────────────────────────────────────────────
Latency (one-way)   < 150 ms      < 100 ms  < 50 ms
Jitter              < 30 ms       < 20 ms   < 10 ms
Packet loss         < 1%          < 0.5%    < 0.1%
Bandwidth           85 Kbps       100 Kbps  128 Kbps (per call)
```

**Video conferencing:**
```
Quality     Resolution    Bandwidth    Latency     FPS
──────────────────────────────────────────────────────
SD          640×480       384 Kbps     < 150 ms    15
HD 720p     1280×720      1.2 Mbps     < 100 ms    30
Full HD     1920×1080     2-4 Mbps     < 80 ms     30
4K          3840×2160     8-12 Mbps    < 50 ms     30
```

## 8.3 MTU e frammentazione

### MTU (Maximum Transmission Unit)

**Problema fragmentation:**
```
Standard Ethernet MTU: 1500 bytes

Packet 1500 bytes → VPN adds 60 bytes overhead → 1560 bytes
├─ Exceeds MTU!
└─ Must fragment → Performance degradation

Fragmented packets:
├─ Fragment 1: 1500 bytes
└─ Fragment 2: 60 bytes
   ↓
   More packets, more overhead, higher loss probability
```

**Path MTU Discovery (PMTUD):**
```
Client → Packet with DF (Don't Fragment) bit → Internet → Gateway
                                                   │
                                      MTU exceeded │
                                                   ↓
Client ← ICMP "Fragmentation Needed" (MTU=1400) ← Gateway
   │
   └─ Adjust MSS/MTU
```

### MSS Clamping

**MSS (Maximum Segment Size)** = MTU - IP header (20) - TCP header (20)

```
Standard:
MTU 1500 → MSS 1460

With VPN (overhead 60):
MTU 1500 → VPN overhead 60 → Effective 1440
MSS should be: 1440 - 40 = 1400
```

**Configurazione MSS clamping:**

*IPsec (strongSwan):*
```bash
# /etc/strongswan.d/charon.conf
charon {
    plugins {
        kernel-netlink {
            mtu = 1400
            mss = 1360
        }
    }
}
```

*OpenVPN:*
```bash
# Server config
tun-mtu 1400
mssfix 1360

# Automatic (recommended)
mssfix 0  # Auto-detect
```

*iptables:*
```bash
# Clamp MSS to PMTU
iptables -t mangle -A FORWARD -p tcp --tcp-flags SYN,RST SYN \
    -j TCPMSS --clamp-mss-to-pmtu

# Clamp to specific value
iptables -t mangle -A FORWARD -p tcp --tcp-flags SYN,RST SYN \
    -j TCPMSS --set-mss 1360
```

*Cisco:*
```cisco
interface Tunnel0
 ip tcp adjust-mss 1360
```

### Optimal MTU determination

**Test MTU:**
```bash
# Linux/macOS
# Try different sizes (don't fragment)
ping -M do -s 1472 vpn.company.com  # 1472 + 28 (headers) = 1500
ping -M do -s 1400 vpn.company.com
ping -M do -s 1350 vpn.company.com

# Windows
ping -f -l 1472 vpn.company.com

# Find largest size that doesn't fragment
# Binary search: start 1500, if fails try 1400, then 1450, etc.
```

**Script auto-detect:**
```bash
#!/bin/bash
# mtu_test.sh
HOST=$1
MIN=1200
MAX=1500

while [ $MIN -lt $MAX ]; do
    MID=$(( ($MIN + $MAX + 1) / 2 ))
    if ping -M do -s $MID -c 3 -W 1 $HOST &>/dev/null; then
        MIN=$MID
    else
        MAX=$(($MID - 1))
    fi
done

echo "Optimal MTU: $(($MIN + 28))"
echo "Optimal MSS: $MIN"
```

### Jumbo Frames

Per reti interne, aumentare MTU può migliorare performance.

```bash
# Set MTU 9000 (jumbo frames)
ip link set dev eth0 mtu 9000

# Persistent (netplan Ubuntu)
cat > /etc/netplan/01-netcfg.yaml << EOF
network:
  version: 2
  ethernets:
    eth0:
      mtu: 9000
EOF

netplan apply
```

**Benefits:**
- Meno pacchetti per stesso volume dati
- Meno CPU cycles (meno header processing)
- Migliore throughput (~10-20%)

**Requirements:**
- Tutto il path deve supportare MTU maggiore
- Switch, router, NIC devono supportare jumbo frames

## 8.4 Ottimizzazione delle performance

### Tuning del sistema operativo

#### Linux kernel tuning

```bash
# /etc/sysctl.conf

# Increase network buffers
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.core.rmem_default = 16777216
net.core.wmem_default = 16777216
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728

# TCP optimization
net.ipv4.tcp_congestion_control = bbr
net.ipv4.tcp_notsent_lowat = 16384
net.ipv4.tcp_slow_start_after_idle = 0

# Increase connection tracking
net.netfilter.nf_conntrack_max = 1048576
net.netfilter.nf_conntrack_tcp_timeout_established = 600

# Network device queue
net.core.netdev_max_backlog = 5000

# Apply settings
sysctl -p
```

#### OpenVPN tuning

```bash
# Server config optimization

# Increase buffer sizes
sndbuf 393216
rcvbuf 393216
push "sndbuf 393216"
push "rcvbuf 393216"

# Optimize fragment/MSS
fragment 0  # Disable (let TCP handle)
mssfix 0    # Auto

# Fast I/O
fast-io

# Disable compression if CPU limited
comp-lzo no

# Multi-threading (>= 2.4)
# Use multiple CPU cores
management localhost 7505

# Optimize cipher
cipher AES-128-GCM  # Faster than AES-256 with minimal security trade-off
```

#### IPsec tuning

```bash
# strongSwan optimization
# /etc/strongswan.d/charon.conf

charon {
    # Multi-threading
    threads = 16
    
    # DNS timeout
    dns_timeout = 5000
    
    # Packet processing
    process_route = no
    install_routes = yes
    
    plugins {
        kernel-netlink {
            # Roam
            roam_events = yes
        }
    }
}
```

### Hardware acceleration

**AES-NI check and enable:**
```bash
# Check support
cat /proc/cpuinfo | grep aes

# Verify OpenSSL uses AES-NI
openssl speed -evp aes-256-gcm

# Output should show high throughput (> 1 GB/s)
```

**Crypto hardware accelerators:**
- Intel QuickAssist Technology (QAT)
- Cavium NITROX
- Broadcom BCM58xx

**Setup Intel QAT:**
```bash
# Install driver
apt install qat-driver qat-utils

# Check status
qatmgr --status

# Configure strongSwan to use QAT
# /etc/strongswan.d/charon/kernel-qat.conf
kernel-qat {
    load = yes
}
```

### Load balancing

**Multiple VPN gateways:**
```
         DNS Round Robin
    vpn1.com     vpn2.com     vpn3.com
        │            │            │
    ┌───▼────┐   ┌───▼────┐   ┌───▼────┐
    │  GW1   │   │  GW2   │   │  GW3   │
    │  33%   │   │  33%   │   │  33%   │
    └────┬───┘   └────┬───┘   └────┬───┘
         └────────┴────────┴────────┘
                  │
            Internal LAN
```

**HAProxy for VPN (Layer 4 LB):**
```
frontend vpn_front
    bind *:1194
    mode tcp
    option tcplog
    default_backend vpn_back

backend vpn_back
    mode tcp
    balance leastconn
    option tcp-check
    
    server vpn1 10.0.1.10:1194 check inter 5s rise 2 fall 3
    server vpn2 10.0.1.11:1194 check inter 5s rise 2 fall 3
    server vpn3 10.0.1.12:1194 check inter 5s rise 2 fall 3
```

### Compression

**Quando usare:**
- Bandwidth limitata
- Traffico comprimibile (HTTP, text)
- CPU non è bottleneck

**Quando NON usare:**
- Traffico già compresso (HTTPS, video, ZIP)
- CPU limitata
- High-throughput scenario

**OpenVPN compression:**
```bash
# LZ4 (recommended - fast)
comp-lzo no
compress lz4-v2
push "compress lz4-v2"

# LZO (legacy)
comp-lzo adaptive

# Disable compression
comp-lzo no
push "comp-lzo no"
```

**IPsec compression (IPComp):**
```bash
# strongSwan
conn myVPN
    compress=yes  # Enable IPComp
```

**Benchmark compression:**
```
Test: Transfer 100 MB text file

No compression:      120 seconds (833 KB/s)
LZ4 compression:     45 seconds (2.2 MB/s)  ← 2.67x faster
CPU usage:           +15%

Test: Transfer 100 MB JPEG images

No compression:      130 seconds
LZ4 compression:     135 seconds  ← Slower! (overhead)
CPU usage:           +20%

Conclusion: Use compression selectively
```

## 8.5 Monitoring e troubleshooting

### Monitoring tools

#### Server-side monitoring

**System metrics:**
```bash
# CPU usage per process
top -p $(pgrep openvpn)
htop -p $(pgrep openvpn)

# Network throughput
iftop -i tun0
nload -i tun0

# Connection tracking
watch -n 1 'cat /proc/net/nf_conntrack | wc -l'

# OpenVPN status
tail -f /var/log/openvpn/openvpn-status.log

# IPsec SA status
watch -n 5 'ipsec statusall'
```

**Netdata monitoring:**
```bash
# Install Netdata
bash <(curl -Ss https://my-netdata.io/kickstart.sh)

# Access dashboard
http://server-ip:19999

# Monitors:
# - CPU, RAM, Network I/O
# - Per-interface statistics
# - VPN connections
# - QoS metrics
```

#### Client-side troubleshooting

**Connection speed test:**
```bash
# Via VPN
curl -s https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py | python3 -

# Results:
# Download: 85.43 Mbit/s
# Upload: 23.12 Mbit/s
# Ping: 32 ms
```

**Packet loss test:**
```bash
# MTR (My TraceRoute)
mtr -r -c 100 192.168.1.1

# Output shows:
# - Each hop latency
# - Packet loss %
# - Jitter
```

**VPN throughput:**
```bash
# Download test from internal server
wget http://192.168.1.100/testfile.bin -O /dev/null

# Results:
# 2024-01-15 10:30:00 (8.5 MB/s) - '/dev/null' saved [104857600/104857600]
```

### Performance bottleneck identification

**Diagnosis flowchart:**
```
Low throughput?
    │
    ├─ Check CPU usage
    │   └─ High (>80%)? → Add cores, enable AES-NI, use lighter cipher
    │
    ├─ Check network bandwidth
    │   └─ Saturated? → Upgrade link, add gateways (load balance)
    │
    ├─ Check latency
    │   └─ High (>100ms)? → Check routing, ISP issues, geographic distance
    │
    ├─ Check packet loss
    │   └─ >1%? → Check MTU, network quality, congestion
    │
    └─ Check VPN config
        └─ MTU issues? → Adjust MTU/MSS
        └─ Compression enabled incorrectly? → Disable for binary data
        └─ TCP-over-TCP? → Switch to UDP
```

**Example troubleshooting:**
```bash
# Scenario: Slow VPN performance

# 1. Check CPU
top
# openvpn process at 95% CPU → Bottleneck found!

# 2. Verify AES-NI
grep aes /proc/cpuinfo
# No output → AES-NI not available

# 3. Solution: Switch to ChaCha20
# Edit server.conf:
# cipher AES-256-GCM → cipher CHACHA20-POLY1305

# 4. Restart and test
systemctl restart openvpn-server@server
iperf3 -c internal-server

# Result: Throughput improved from 150 Mbps to 450 Mbps
```

### Logging e analytics

**Structured logging:**
```bash
# OpenVPN JSON logging
# Custom script: /etc/openvpn/client-connect.sh
#!/bin/bash
echo "{
  \"timestamp\": \"$(date -Iseconds)\",
  \"event\": \"client_connect\",
  \"user\": \"$common_name\",
  \"ip\": \"$trusted_ip\",
  \"vpn_ip\": \"$ifconfig_pool_remote_ip\"
}" | tee -a /var/log/openvpn/connections.json

# Configure in server.conf
script-security 2
client-connect /etc/openvpn/client-connect.sh
```

**ELK Stack integration:**
```bash
# Filebeat configuration
# /etc/filebeat/filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/openvpn/*.log
  json.keys_under_root: true
  json.add_error_key: true

output.elasticsearch:
  hosts: ["localhost:9200"]
  index: "vpn-logs-%{+yyyy.MM.dd}"
```

**Grafana dashboard:**
```
Panels:
1. Active connections (time series)
2. Bandwidth usage (stacked graph)
3. Connection duration (histogram)
4. Top users by traffic (table)
5. Geographic map of connections
6. Authentication failures (counter)
```

---

**Capitolo precedente**: [07. Configurazione e Implementazione](07.Configurazione_e_Implementazione.md)  
**Prossimo capitolo**: [09. Applicazioni Pratiche](09.Applicazioni_Pratiche.md)
